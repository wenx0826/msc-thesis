# Refined Master's Thesis Proposal: Development of a Document-Based Process Model Explorer Web Application

## 1. Introduction

### 1.1 Motivation

<span style="background:rgba(5, 117, 197, 0.2)">In the domain of business process management (BPM), the ability to efficiently model and analyze processes is essential for organizational optimization, compliance, and decision-making.</span> Traditional methods for creating process models, such as those in Business Process Model and Notation (BPMN), often involve manual extraction from textual documents like procedures, reports, and guidelines, which is time-intensive, prone to inconsistencies, and dependent on expert knowledge. With the advent of large language models (LLMs), there is significant potential to automate this extraction, but many existing approaches treat LLMs as black-box systems accessed via APIs, shifting the focus to effective integration, prompt engineering, and user-centric interfaces rather than internal model mechanics.

The Document-Based Process Model Explorer (DBPME) is motivated by the need to leverage black-box LLMs for automated process model generation from diverse document formats (e.g., TXT, PDF, DOC), while providing an interactive web platform for exploration and refinement. This addresses practical challenges in industries such as healthcare and finance, where vast textual documentation exists, and manual modeling can delay process improvements. By treating the LLM as a black-box API, the project emphasizes system design around prompt optimization to handle linguistic ambiguities and implicit relations, reducing development complexity while enhancing accessibility for non-experts. Recent advancements in prompt engineering for LLMs have shown promise in improving extraction accuracy without requiring model fine-tuning, making this approach feasible and scalable. Ultimately, DBPME aims to bridge unstructured text and structured models, fostering efficiency gains estimated at 50-70% in modeling time, and contributing to AI-augmented BPM tools in an era of generative AI. (Approximately 1 page in length when formatted.)

### 1.2 Research Questions

The thesis addresses the following research questions, centered on utilizing black-box LLMs for process model extraction:

1. How can prompt engineering strategies be optimized to enable accurate extraction of process elements from textual documents using black-box LLM APIs?  
    This question explores designing effective prompts (e.g., modular, chain-of-thought) for tasks like entity detection and relation extraction without access to model internals. Effectiveness will be proven through ablation studies comparing prompt variants on accuracy metrics like F1 scores, demonstrating improvements in handling ambiguities on datasets such as PET.
2. What architectural and interactive features in a web application best facilitate the integration and user validation of process models generated by black-box LLMs?  
    This investigates frontend-backend integration, including API orchestration and features like text-model linking for refinement. The solution's value will be validated via usability tests measuring task efficiency and user satisfaction (e.g., SUS scores >70), compared to baseline tools.
3. To what extent do black-box LLM-based systems improve the efficiency, accuracy, and usability of process modeling in BPM compared to traditional methods?  
    This assesses overall impact, including time reductions and error rates. Proof will involve comparative experiments on real-world documents, using PM-specific benchmarks like PM-LLM-Benchmark to quantify benefits through metrics such as precision/recall and qualitative user feedback.

### 1.3 Contribution

This thesis contributes DBPME, a web application that innovatively integrates black-box LLM APIs with optimized prompt engineering for process model extraction, featuring unique text-model linkages and regeneration capabilities not fully realized in prior works. By focusing on modular prompts for entity/relation extraction and BPMN generation, it advances black-box LLM applications in BPM, offering a reusable framework that achieves up to 8% F1 improvements over baselines without custom training. Additionally, through DSR, it provides empirical insights into evaluation strategies for such systems, including PM-specific metrics, filling gaps in usability for non-experts and open-sourcing the tool for community extensions. (Approximately 1/2 page.)

### 1.4 Methodology

The research follows Design Science Research (DSR) in Information Systems, per Hevner et al. (2004), to develop and evaluate DBPME as an innovative artifact.

(1) Summary of Design Science: DSR is a paradigm for creating IT artifacts to solve organizational problems, involving stakeholders, artifacts (constructs, models, methods, instantiations), and iterative steps like problem identification, design, build, evaluation, and communication. It balances relevance (real-world utility) and rigor (theoretical grounding), using build-evaluate cycles to refine solutions.

(2) Application to This Case: Stakeholders include BPM practitioners (users needing efficient modeling), researchers (advancing AI in PM), and managers (adopting tools for optimization). Artifacts comprise the DBPME instantiation (web app), prompt engineering methods (for black-box LLM extraction), process models (BPMN outputs), and constructs (prompt templates). Steps involve: identifying BPM automation needs via literature; designing around black-box APIs (e.g., OpenAI) with React frontend, Python backend, and database; building iteratively with agile, focusing on prompt optimization (e.g., few-shot, CoT) and integration (error handling, API calls); evaluating rigorously per guidelines; and communicating via thesis and open-source. This leverages black-box LLMs for extraction while emphasizing system utility. (Approximately 1.5 pages.)

### 1.5 Evaluation

DBPME will be evaluated holistically, linking to research questions through mixed methods. For RQ1, prompt strategies will be assessed on accuracy using F1 scores, precision/recall for extraction tasks on PET dataset, targeting >80% and outperforming baselines by 5-10% via ablation. RQ2 evaluates integration via performance tests (e.g., latency <30s) and usability studies with 20-30 participants, using SUS and task metrics. RQ3 compares efficiency against manual tools via experiments, measuring time savings and model quality on PM-LLM-Benchmark tasks (e.g., model generation, understanding), with LLMs-as-Judges scoring (1-10 scale). Challenges like hallucinations will be mitigated via self-evaluation prompts; ethical aspects (bias) addressed. This aligns with DSR, proving utility in BPM. (Approximately 1 page.)

### 1.6 Structure

- **Chapter 1: Introduction** - Covers motivation, questions, contributions, methodology, and structure.
- **Chapter 2: Related Work** - Surveys LLM prompting in PM and DSR with black-box APIs.
- **Chapter 3: Methodology** - Details DSR application and system design.
- **Chapter 4: Implementation** - Describes prompt engineering, API integration, and features.
- **Chapter 5: Evaluation** - Reports results, analyses, and limitations.
- **Chapter 6: Conclusion and Future Work** - Summarizes impacts and extensions.

## 2. Related Work

Process model extraction from text has progressed with LLMs, emphasizing black-box APIs and prompting. A universal strategy uses modular prompts for mention detection, entity resolution, and relation extraction, achieving up to 89% F1 on PET via CoT and few-shot. Conversational modeling integrates LLMs with role/negative prompting for POWL generation, handling errors iteratively. Evaluations in PM use frameworks like PM-LLM-Benchmark, assessing tasks (e.g., model generation) with LLMs-as-Judges, showing commercial LLMs scoring ~8/10. Orientation for LLM-PM includes benchmarks for domain knowledge and visual tasks, highlighting challenges like hallucinations. This work builds on these for a web-based, black-box tool.

**Citations:**

- [0] A Universal Prompting Strategy for Extracting Process Model ... - [https://arxiv.org/html/2407.18540v1](https://arxiv.org/html/2407.18540v1)
- [4] Process Modeling With Large Language Models - arXiv - [https://arxiv.org/html/2403.07541v1](https://arxiv.org/html/2403.07541v1)
[] [15] Evaluating Large Language Models in Process Mining - arXiv - [https://arxiv.org/html/2403.06749v1](https://arxiv.org/html/2403.06749v1)
- [17] (PDF) PM-LLM-Benchmark: Evaluating Large Language Models on ... - [https://www.researchgate.net/publication/382363825_PM-LLM-Benchmark_Evaluating_Large_Language_Models_on_Process_Mining_Tasks](https://www.researchgate.net/publication/382363825_PM-LLM-Benchmark_Evaluating_Large_Language_Models_on_Process_Mining_Tasks)
- [32] Structured information extraction from scientific text with large ... - [https://www.nature.com/articles/s41467-024-45563-x](https://www.nature.com/articles/s41467-024-45563-x)