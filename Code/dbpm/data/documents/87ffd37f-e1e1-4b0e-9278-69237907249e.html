# A Proposal for a Document-based Process Model Explorer

## 1. Introduction

### 1.1 Motivation

A significant portion of any organization's operational knowledge is not stored in structured databases but is instead embedded within a vast and growing collection of unstructured documents. 1 Manuals for standard operating procedures (SOPs), internal policy documents, and project reports all contain detailed descriptions of how work is actually performed. The manual extraction and formal modeling of this "latent" knowledge is the primary bottleneck in the Business Process Management (BPM) lifecycle, proving to be time-consuming, expensive, and susceptible to human error. 2  

[Automated Business Process Discovery from Unstructured Natural-Language Documents | Request PDF - ResearchGate](https://www.researchgate.net/publication/350396262_Automated_Business_Process_Discovery_from_Unstructured_Natural-Language_Documents)
[researchgate.net/publication/350396262_Automated_Business_Process_Discovery_from_Unstructured_Natural-Language_Documents](https://www.researchgate.net/publication/350396262_Automated_Business_Process_Discovery_from_Unstructured_Natural-Language_Documents)

[Automated Business Process Discovery from Unstructured Natural-Language Documents](https://openreview.net/forum?id=j8esuGIJTH)

[What is Process Discovery? | Nintex](https://www.nintex.com/learn/process-management/what-is-process-discovery/)

[

![信息来源图标](https://t0.gstatic.com/faviconV2?url=https://www.nintex.com/&client=BARD&type=FAVICON&size=256&fallback_opts=TYPE,SIZE,URL)

nintex.com/learn/process-management/what-is-process-discovery

](https://www.nintex.com/learn/process-management/what-is-process-discovery/)

[

Automated Process Discovery Explained - Methods, Tools & Tips | ProcessMaker

](https://www.processmaker.com/blog/automated-process-discovery-explained/)

[

![信息来源图标](https://t1.gstatic.com/faviconV2?url=https://www.processmaker.com/&client=BARD&type=FAVICON&size=256&fallback_opts=TYPE,SIZE,URL)

processmaker.com/blog/automated-process-discovery-explained

](https://www.processmaker.com/blog/automated-process-discovery-explained/)

[

System Usability Scale (SUS) Practical Guide for 2025 - UXtweak

](https://blog.uxtweak.com/system-usability-scale/)

[

![信息来源图标](https://t3.gstatic.com/faviconV2?url=https://blog.uxtweak.com/&client=BARD&type=FAVICON&size=256&fallback_opts=TYPE,SIZE,URL)

blog.uxtweak.com/system-usability-scale

](https://blog.uxtweak.com/system-usability-scale/)

The advent of powerful Large Language Models (LLMs), now accessible through APIs, presents a transformative opportunity to automate the initial translation from text to a formal process model, such as the CPEE model. These services can process a natural language description and generate a structured output, drastically reducing the initial modeling effort. However, treating these powerful APIs as infallible "black box" solutions is a critical mistake. The probabilistic nature of LLMs means their output, while often impressive, is susceptible to "hallucinations"—generating plausible but factually incorrect or logically inconsistent information. A single misplaced element or an illogical sequence flow can render an entire process model invalid and unusable.  

This creates a new and urgent problem: while the barrier to _generating_ a process model has been lowered, the challenge of _validating and trusting_ this AI-generated output remains. A business analyst or domain expert receiving a model from an API has no direct way to verify its faithfulness to the source text. Without a clear link between the textual description and the resulting diagram, the model is opaque, its origins are untraceable, and correcting subtle errors becomes a frustrating exercise of guesswork. The API accelerates the first draft, but it does not solve the fundamental need for human oversight, contextual correction, and validation to ensure the final model accurately reflects reality.

This project, the "Document-based Process Model Explorer," is motivated by this critical gap between automated generation and human validation. The core innovation is not in building another generative model, but in designing and evaluating a **human-in-the-loop system** that makes the output of an external, black-box AI service transparent, verifiable, and refinable. As envisioned in the mockup, the Explorer will create an **interactive visual analytics environment** that allows a user to visually trace the connections between source text fragments and the corresponding model elements generated by the API. This interactive paradigm directly confronts the limitations of opaque, one-shot generation by empowering the user to become an active participant in the sensemaking and validation process, ensuring the final model is both formally correct and semantically faithful to the original knowledge.  

### 1.2 Research Questions

This thesis is guided by the following research questions, which focus on the integration and evaluation of a generative API within a novel interactive system targeting the CPEE model notation.

- **RQ1: How accurately can a state-of-the-art, third-party generative AI API generate syntactically and semantically correct CPEE process models from unstructured procedural texts?** This question establishes a performance baseline for the "black box" component of the system. It moves beyond a simple "yes/no" to investigate the _quality_ of the raw, automated output. To answer this, a quantitative evaluation will be performed. A "gold standard" dataset of text-model pairs will be created. The API's generated models will be systematically compared against this standard using metrics like precision, recall, F1-score for model elements, and graph-edit distance for structural similarity. This provides an objective measure of the API's out-of-the-box capabilities and quantifies the "problem" that the interactive system aims to solve.  
    
- **RQ2: How can an interactive visual interface, featuring bidirectional linking between source text and model elements, improve a user's ability to effectively validate and correct the output of an automated process model generation API?** This question focuses on the core contribution of the thesis: the interactive system itself. It seeks to measure the value added by the "glass box" interface in overcoming the opaqueness of the API. The answer will be derived from a formal user study. A key metric will be the measurable improvement in model quality (re-evaluated against the gold standard) after a user has performed corrections using the system, compared to the baseline quality from RQ1. This will be supplemented by task completion rates and qualitative feedback.  
    
- **RQ3: What specific interactive features and visual cues are most effective in facilitating the cognitive process of aligning textual descriptions with their corresponding API-generated CPEE model elements?** This question delves deeper into the HCI aspect, aiming to identify which design choices contribute most to the system's effectiveness. While RQ2 asks _if_ the interface helps, RQ3 asks _how_ it helps. This will be investigated through the qualitative analysis of the user study data. By analyzing user behaviors, comments from the think-aloud protocol, and interview responses, we can identify which features (e.g., bidirectional highlighting, the properties panel, the regeneration feedback loop) were most crucial for users to identify and fix errors in the API's output.
    

### 1.3 Contribution

This research will make three primary contributions to the fields of Business Process Management (BPM) and Human-Computer Interaction (HCI), focusing on the effective application of generative AI.

1. **A Novel System Architecture for Human-AI Collaboration:** The project will deliver the design and implementation of an integrated system that effectively "wraps" a powerful but imperfect generative AI service with a user-centric interactive front-end. The contribution is the blueprint for a complete, end-to-end workflow that transforms an opaque API call into a transparent and trustworthy modeling process.
    
2. **An Interactive Validation and Refinement Paradigm:** The core innovation is the development and application of a bidirectional synchronization mechanism that visually links textual phrases to the CPEE model elements generated by an external API. This feature provides a novel method for model validation and traceability, creating a "glass box" that allows users to understand the basis for the AI's output. This moves beyond the state-of-the-art by directly addressing the critical problem of trust and verifiability in AI-generated artifacts.
    
3. **A Rigorous Empirical Evaluation of a Human-in-the-Loop System:** The thesis will provide a comprehensive evaluation that first benchmarks the performance of a black-box generative API for process modeling (RQ1), and then empirically demonstrates the value added by the interactive system in improving the quality of those models (RQ2 & RQ3). This dual evaluation provides a holistic validation of the proposed human-in-the-loop solution.
    

### 1.4 Methodology

This project will be conducted using the **Design Science Research Methodology (DSRM)**, a well-established framework for research in Information Systems. DSR is perfectly suited for this project as its primary goal is the creation and evaluation of a novel IT artifact designed to solve a relevant, real-world problem—in this case, the problem of validating and refining models generated by opaque AI services.  

**(1) Summary of Design Science Research**

Design Science Research is a problem-solving paradigm focused on creating innovative artifacts to address specific challenges. Unlike behavioral science, which seeks to  

_explain_ reality, design science aims to _create_ useful things. The process is inherently iterative, cycling through the activities of building an artifact and evaluating its performance in a given context. Hevner et al. describe this through three interconnected cycles: the  

_Relevance Cycle_ (connecting to the problem environment), the _Rigor Cycle_ (grounding in existing knowledge), and the _Design Cycle_ (the core build-and-evaluate loop). The outputs of DSR are the artifact itself and the knowledge gained in the process.  

**(2) Application of DSRM to this Project**

- **Stakeholders:** The primary stakeholders are individuals involved in managing business processes who stand to benefit from more efficient and accessible modeling tools. This includes:  
    
    - **Business Analysts and Process Modelers:** Who will use the tool to accelerate their work by quickly validating and correcting API-generated drafts.
        
    - **Domain Experts / Subject Matter Experts (SMEs):** Who can use the intuitive interface to validate models without needing deep expertise in formal modeling.
        
    - **Managers:** Who rely on accurate process models for decision-making.
        
- **Artifacts:** The research will produce several artifacts consistent with the DSR framework:  
    
    - **Models:** The high-level system architecture that integrates an external generative API with an interactive frontend via a backend orchestration layer.
        
    - **Methods:** The prompt engineering strategies used to instruct the external API to return not only the process model but also the necessary source mapping data.
        
    - **Instantiation:** The final, implemented "Document-based Process Model Explorer" web application. This application is the primary artifact and the object of evaluation.
        
- **Steps:** The research will follow the six-step DSRM process model:  
    
    1. **Problem Identification and Motivation:** This is detailed in Section 1.1. The problem is the lack of trust, transparency, and verifiability in process models generated by black-box AI APIs.
        
    2. **Define Objectives for a Solution:** The objectives are to create a system that (a) seamlessly integrates with a third-party generation API, (b) makes the API's output transparent through interactive visualization, and (c) enables users to efficiently validate and correct the generated model.
        
    3. **Design and Development:** This phase involves designing the system architecture and implementing the backend API orchestration layer and the frontend visualization interface. The focus is on the _system_, not the internal AI model.
        
    4. **Demonstration:** The artifact will be used to solve the problem, showcasing the workflow of uploading a document, receiving an API-generated model, and using the interactive features to validate and refine it.
        
    5. **Evaluation:** This crucial step involves rigorously evaluating the artifact against the objectives, as detailed in the Evaluation section (1.5).
        
    6. **Communication:** The results will be communicated through the master's thesis document.
        

### 1.5 Evaluation

A robust, two-part evaluation strategy will be employed to rigorously assess the developed artifact and answer the research questions. A ground-truth dataset will be established, consisting of 10-15 procedural text documents and their corresponding "gold standard" CPEE models, manually created by an expert.

**Part 1: Baseline Performance Analysis of the Generative API (for RQ1)**

This evaluation will objectively measure the quality of the raw output from the external, black-box API. For each document in the test dataset, a CPEE model will be generated via an API call. This model will then be systematically compared against the gold standard using established metrics.

- **Element-level Analysis:** The set of generated elements (tasks, gateways) will be compared to the gold standard to calculate **Precision, Recall, and F1-score**.
    
- **Structural Analysis:** The graph structure of the generated model will be compared to the gold standard using a **graph-edit distance** algorithm to produce a quantitative score of their structural similarity.  
    

**Part 2: Evaluation of the Interactive Explorer's Effectiveness (for RQ2 & RQ3)**

This evaluation will assess the value added by the interactive system from a human user's perspective. A formal user study will be conducted with 10-15 participants.

- **Task-based Scenarios:** Participants will be given the raw, API-generated models and the source texts. Their task will be to use the "Document-based Process Model Explorer" to find and correct any discrepancies.
    
- **Effectiveness Measurement:** The primary metric will be the **improvement in model quality**. The corrected models produced by the users will be compared against the gold standard using the same F1-score and graph-edit distance metrics from Part 1. A significant improvement over the baseline API score will demonstrate the effectiveness of the system.
    
- **Usability Measurement:** After completing the tasks, each participant will complete the **System Usability Scale (SUS)** questionnaire, a reliable industry standard that yields a single score from 0 to 100 representing perceived usability.  
    
- **Qualitative Feedback:** A think-aloud protocol and post-session interviews will be used to gather in-depth feedback on which interactive features were most helpful for identifying and fixing errors, directly addressing RQ3.
    

### 1.6 Structure

The final thesis will be organized into the following chapters:

- **Chapter 1: Introduction:** Presents the motivation, research questions, contributions, and methodology.
    
- **Chapter 2: Related Work:** Reviews the state of the art in process modeling with the CPEE notation, text-based process discovery, and interactive visual analytics for human-AI collaboration.
    
- **Chapter 3: System Design and Architecture:** Details the design of the "Document-based Process Model Explorer," focusing on the backend API orchestration layer and the frontend interactive interface.
    
- **Chapter 4: Implementation:** Describes the implementation of the system, including the technologies, libraries, and the prompt engineering strategies developed for the API.
    
- **Chapter 5: Evaluation:** Presents the methodology and results of the two-part evaluation: the baseline API performance analysis and the user study of the interactive system.
    
- **Chapter 6: Discussion:** Interprets the evaluation results, discusses their implications for the research questions, and acknowledges limitations.
    
- **Chapter 7: Conclusion and Future Work:** Summarizes the contributions and proposes directions for future research.
    

## 2. Related Work

### 2.1 Foundations of the CPEE Process Model Notation

The Cloud Process Execution Engine (CPEE) is a modular, service-oriented workflow engine designed to be a lightweight, scalable, and powerful alternative to traditional, monolithic Business Process Management Systems. While the engine is capable of executing models from various standard languages like BPMN , it is optimized for its own internal process representation, often referred to as a CPEE Tree.  

For user-facing modeling, as demonstrated in the CPEE Cockpit and related tools, the engine employs a graphical notation that is visually similar to established standards like BPMN. It uses familiar elements such as tasks (activities), events (start/end points), and gateways (decision points) connected by sequence flows. This approach effectively lowers the barrier to entry for users already familiar with conventional process modeling. However, the underlying semantics and structure are tailored for the CPEE's flexible, service-oriented, and highly distributed architecture, prioritizing direct executability and runtime adaptability over descriptive richness. A foundational understanding of these core visual elements is essential, as they form the target structure that the proposed system's NLP engine must extract from text.  

### 2.2 The Evolution of Process Discovery: From Event Logs to Text Mining

The field of process discovery has traditionally been dominated by Process Mining, which analyzes structured event logs from enterprise systems. However, a key limitation of process mining is its dependence on these logs, as much process knowledge exists only in unstructured documents. This has spurred the growth of Automated Business Process Discovery (ABPD) from text, which applies NLP to discover "hidden" processes. This thesis is situated in this area, focusing specifically on the challenges and opportunities of transforming unstructured document-based knowledge into formally structured and analyzable CPEE models.  

### 2.3 A Survey of NLP Techniques for Process Element Extraction

The technical challenge of transforming natural language into the structured elements of a CPEE model has seen significant evolution. Early systems used brittle rule-based methods, followed by supervised machine learning techniques like Named Entity Recognition (NER) that required expensive, manually annotated datasets. The recent emergence of Large Language Models (LLMs), accessible via APIs, has catalyzed a paradigm shift, enabling end-to-end text-to-model conversion. However, the probabilistic nature of these models makes them prone to "hallucinations" and the generation of syntactically invalid output. To mitigate this, a  

**hybrid approach** has emerged, where an LLM is used for semantic understanding to produce a structured intermediate format (e.g., JSON), and a deterministic algorithm constructs the final, valid process model. This project assumes the external API performs a similar function, but acknowledges the inherent risks of this approach, which motivates the need for an interactive validation layer to ensure the final output is a formally correct and executable CPEE model.  

### 2.4 Paradigms for Interactive Process Model Analysis and Refinement

The automated generation of a process model is only the first step. The model must be explored, validated, and refined by domain experts. The proposed system's interactive features are grounded in established principles from  

**Visual Analytics (VA)** and **Interactive Machine Learning**. VA is the science of analytical reasoning facilitated by interactive visual interfaces, emphasizing the synergy between automated analysis and human cognition. The core VA principle of "Analyze First, Show the Important, Zoom, Filter and Analyze Further, Details on Demand" provides a theoretical foundation for the Explorer's design. Similarly, Interactive Machine Learning explores systems where users provide feedback to iteratively guide and refine a model's behavior. The "Regenerate" function envisioned in the mockup is a direct application of this paradigm, creating a feedback loop that transforms the user from a passive consumer of an API's output into an active co-creator of a validated, trustworthy model.