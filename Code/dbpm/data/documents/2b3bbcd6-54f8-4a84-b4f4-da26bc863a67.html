
How will I evaluate that my proposal is good. This ties into the research questions.

### Gemini v1

To rigorously answer the research questions and validate the contributions of this thesis, a multi-faceted evaluation strategy will be employed. This strategy is designed to first establish the context and novelty of the work (addressing RQ1) and then empirically measure the usability and impact of the developed artifact (addressing RQ2 and RQ3).

**Answering RQ1: Systematic Literature Review**

RQ1 will be answered through a **Systematic Literature Review**. This involves a structured survey of academic and industry literature to:
*   Identify and categorize existing tools and approaches for business process modeling, from manual editors to automated text-to-model systems.
*   Analyze the documented performance and challenges of using Generative AI for this task, with a specific focus on the well-known issues of "hallucinations," lack of traceability, and the resulting trust deficit.
*   Explore the interaction paradigms of other document-centric AI tools to identify novel design patterns.

The successful outcome of this review will be a clear articulation of the research gap that the DBPME, as depicted in the mockup, is designed to fill.

**Answering RQ2 & RQ3: Formal User Study**

RQ2 and RQ3 will be answered through a **formal, comparative user study**. This study will be designed to measure the practical value of the DBPME artifact.

*   **Participants:** The study will recruit 10-15 participants from the target user groups, which include requirements engineers and process designers (or suitable proxies such as graduate students in computer science or business informatics).

*   **Study Design and Conditions:** A within-subjects design will be used, where each participant completes modeling tasks under two conditions:
    1.  **The DBPME Condition:** Participants will use the implemented Document-Based Process Model Explorer, which reflects the design in the mockup, to generate, validate, and refine a process model from a given text.
    2.  **The Traditional Workflow (Control):** Participants will be given the same text document and asked to create a process model from scratch using a standard, non-interactive modeling tool and a separate text editor.

*   **Materials:**
    *   A functional prototype of the **DBPME**, implementing the key features shown in the mockup, such as the three-panel layout and the bidirectional visual linking.
    *   A set of 3-5 procedural text documents of varying complexity.
    *   A corresponding set of **"gold standard" process models**, created by an expert, which will serve as the objective benchmark for measuring the quality of the models produced by the participants.

*   **Data Collection and Metrics:**

    *   **To Answer RQ2 (Usability, Functionality, and User Experience):** We will collect primarily qualitative data.
        *   **Think-Aloud Protocol:** Participants will be asked to verbalize their thoughts while using the DBPME, providing direct insight into how they use the features shown in the mockup (e.g., the visual linking) to understand and correct the model.
        *   **System Usability Scale (SUS):** After the task, participants will complete the industry-standard SUS questionnaire to provide a quantitative score of the DBPME's perceived usability.
        *   **Semi-Structured Interviews:** Post-session interviews will gather in-depth feedback on the tool's design, functionality, and overall user experience.

    *   **To Answer RQ3 (Overall Effectiveness):** We will collect quantitative and qualitative data to compare the DBPME condition against the traditional workflow.
        *   **Output Quality:** The final models produced by each participant in both conditions will be compared against the gold standard. We will calculate objective metrics such as **precision, recall, and F1-score** for the model elements, and **graph-edit distance** for structural similarity. A higher score in the DBPME condition will prove that the tool enhances output quality.
        *   **Modeling Performance:** To measure the impact on the user's workflow without relying on time, we will assess the **perceived cognitive load** using a validated instrument like the NASA-TLX questionnaire. A lower cognitive load score in the DBPME condition will demonstrate that the tool, particularly the visual linking feature from the mockup, makes the modeling task cognitively easier and more efficient.
        *   **Professional Confidence:** To measure the tool's impact on trust—a key benefit of the transparent design shown in the mockup—we will use a post-task questionnaire. Participants will be asked to rate their confidence in the accuracy of the final model they produced on a Likert scale. Higher confidence ratings in the DBPME condition will provide evidence that the tool's traceability features successfully address the trust deficit inherent in black-box AI systems.